{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "940d632a-1bd4-4e11-b74e-408378efa3e6",
    "_uuid": "a0df0389-5dc5-4f01-8691-be84f88f74ff",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:51.223649Z",
     "iopub.status.busy": "2023-06-23T03:57:51.223282Z",
     "iopub.status.idle": "2023-06-23T03:57:51.230578Z",
     "shell.execute_reply": "2023-06-23T03:57:51.229132Z",
     "shell.execute_reply.started": "2023-06-23T03:57:51.223618Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "43db3389-791d-4804-9718-3e8a0724c25b",
    "_uuid": "db526ed1-e5c2-4baa-ac1a-514141381d27",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:52.405891Z",
     "iopub.status.busy": "2023-06-23T03:57:52.404926Z",
     "iopub.status.idle": "2023-06-23T03:57:52.822266Z",
     "shell.execute_reply": "2023-06-23T03:57:52.821116Z",
     "shell.execute_reply.started": "2023-06-23T03:57:52.405825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,  Activation, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid, relu\n",
    "from tensorflow.keras.layers import Conv1D, LeakyReLU, Concatenate, Multiply, Add, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def scale_up_plus_model(input_shape=(None, 1), output_units=399, num_stages=3):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Pre-upsampling\n",
    "    pre_upsampled = Conv1D(filters=16, kernel_size=8, strides=2, padding='same')(input_layer)\n",
    "\n",
    "    # Initialize previous stage output as pre-upsampling output\n",
    "    prev_output = pre_upsampled\n",
    "\n",
    "    # Convolutional layers\n",
    "    conv_layers = []\n",
    "    for i in range(num_stages):\n",
    "        # Concatenate pre-upsampling output and previous stage output\n",
    "        concat = Concatenate()([prev_output, pre_upsampled])\n",
    "\n",
    "        # Conv1\n",
    "        conv1 = Conv1D(filters=16, kernel_size=16, strides=1, activation=LeakyReLU(alpha=0.9), padding='same')(concat)\n",
    "\n",
    "        # Conv2\n",
    "        conv2 = Conv1D(filters=1, kernel_size=16, strides=1, padding='same')(conv1)\n",
    "\n",
    "        # Element-wise addition of pre-upsampling output and Conv2 output\n",
    "        added = Add()([pre_upsampled, conv2])\n",
    "\n",
    "        # Conv4\n",
    "        conv4 = Conv1D(filters=16, kernel_size=1, strides=1, activation='sigmoid', padding='same')(added)\n",
    "\n",
    "        # Conv3\n",
    "        conv3 = Conv1D(filters=16, kernel_size=1, strides=1, padding='same')(conv1)\n",
    "\n",
    "        # Element-wise multiplication of Conv3 and Conv4 output\n",
    "        multiplied = Multiply()([conv3, conv4])\n",
    "\n",
    "        # Element-wise addition of Conv1 output and multiplied output\n",
    "        added_final = Add()([conv1, multiplied])\n",
    "\n",
    "        # Update previous stage output for the next stage\n",
    "        prev_output = added_final\n",
    "\n",
    "        # Output layer\n",
    "        output = Conv1D(filters=output_units, kernel_size=1, strides=1, padding='same')(added_final)\n",
    "\n",
    "        # Store the output of this stage\n",
    "        conv_layers.append(output)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=conv_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "7bd2d444-de06-4717-aa91-5eae1a3198b4",
    "_uuid": "1299b015-c3e2-40f4-b0f6-3fef85fdec0e",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:52.833647Z",
     "iopub.status.busy": "2023-06-23T03:57:52.832578Z",
     "iopub.status.idle": "2023-06-23T03:57:52.843353Z",
     "shell.execute_reply": "2023-06-23T03:57:52.842417Z",
     "shell.execute_reply.started": "2023-06-23T03:57:52.833607Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the path to the VCTK dataset\n",
    "dataset_path = '/kaggle/input/vctk-dataset/VCTK-Corpus/VCTK-Corpus'\n",
    "\n",
    "# Set the path to save the trained model\n",
    "save_model_path = '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "1c6eaa2b-7da5-4f3f-a063-7a7c9d16886a",
    "_uuid": "4edd0e72-89ee-47d5-85d7-1f6daf2be2ad",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:53.047634Z",
     "iopub.status.busy": "2023-06-23T03:57:53.047346Z",
     "iopub.status.idle": "2023-06-23T03:57:53.051967Z",
     "shell.execute_reply": "2023-06-23T03:57:53.050901Z",
     "shell.execute_reply.started": "2023-06-23T03:57:53.047610Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "aacecd29-c853-43a7-aa9a-bf9d83b18239",
    "_uuid": "7653dd85-3e54-43e2-a67f-37dc110454a5",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:53.288580Z",
     "iopub.status.busy": "2023-06-23T03:57:53.287903Z",
     "iopub.status.idle": "2023-06-23T03:57:54.710031Z",
     "shell.execute_reply": "2023-06-23T03:57:54.709003Z",
     "shell.execute_reply.started": "2023-06-23T03:57:53.288548Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "i=0\n",
    "for root, dirs, files in os.walk(os.path.join(dataset_path, 'wav48')):\n",
    "    if i==200:\n",
    "        break;\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            if i==200:\n",
    "                break;\n",
    "            i+=1\n",
    "            try:\n",
    "                sample_rate, audio_data = wavfile.read(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file: {file_path}\")\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            audio_data = audio_data.astype('float32') / 32767.0  # Normalize audio data to the range [-1, 1]\n",
    "\n",
    "            # Resample the audio data to the desired sample rate (32kHz)\n",
    "            target_sample_rate = 44100  # or 48000\n",
    "            resampled_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=target_sample_rate)\n",
    "    \n",
    "\n",
    "            # Split the resampled data into low-bandwidth signal (WB) and high-bandwidth signal (SWB)\n",
    "            wb_data = resampled_data[::2]\n",
    "            swb_data = resampled_data\n",
    "\n",
    "            # Append the WB and SWB data to the dataset\n",
    "            X.append(wb_data)\n",
    "            y.append(swb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "243c4f0f-2afa-4210-b004-239fd3ca5543",
    "_uuid": "491ae30b-5e1e-469e-ba0f-554d07ae4ec0",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:54.712667Z",
     "iopub.status.busy": "2023-06-23T03:57:54.712294Z",
     "iopub.status.idle": "2023-06-23T03:57:54.718032Z",
     "shell.execute_reply": "2023-06-23T03:57:54.716903Z",
     "shell.execute_reply.started": "2023-06-23T03:57:54.712634Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(X, dtype=object)\n",
    "y = np.array(y, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "6462775e-4004-474c-acf0-e399b5714030",
    "_uuid": "329eda72-d70e-4be8-a152-77f1eb2bdc39",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:54.720216Z",
     "iopub.status.busy": "2023-06-23T03:57:54.719872Z",
     "iopub.status.idle": "2023-06-23T03:57:54.731655Z",
     "shell.execute_reply": "2023-06-23T03:57:54.730565Z",
     "shell.execute_reply.started": "2023-06-23T03:57:54.720184Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of samples before splitting\n",
    "if len(X) == 0 or len(y) == 0:\n",
    "    raise ValueError(\"The dataset contains no samples.\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.14, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "3842acb1-3099-4b8e-87dc-7b9d5e43bcb4",
    "_uuid": "6bca5bac-eb7e-4d88-8eb1-209f296ee511",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:54.735200Z",
     "iopub.status.busy": "2023-06-23T03:57:54.734767Z",
     "iopub.status.idle": "2023-06-23T03:57:54.748429Z",
     "shell.execute_reply": "2023-06-23T03:57:54.747536Z",
     "shell.execute_reply.started": "2023-06-23T03:57:54.735167Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (29,)\n",
      "y_train shape: (171,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "84733bb5-f41a-482b-a5e1-6cfc3875d2ed",
    "_uuid": "86a13153-e15c-4245-9683-7d1a3eba3b78",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:57:54.749933Z",
     "iopub.status.busy": "2023-06-23T03:57:54.749599Z",
     "iopub.status.idle": "2023-06-23T03:58:09.904649Z",
     "shell.execute_reply": "2023-06-23T03:58:09.903742Z",
     "shell.execute_reply.started": "2023-06-23T03:57:54.749902Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess target data to obtain individual class labels\n",
    "unique_labels_train = np.unique([tuple(label) for label in y_train])\n",
    "unique_labels_test = np.unique([tuple(label) for label in y_test])\n",
    "unique_labels = np.unique(np.concatenate((unique_labels_train, unique_labels_test)))  # Obtain unique target values in both training and testing data\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}  # Create a mapping from target value to label index\n",
    "\n",
    "# Map target values to label indices in training data\n",
    "y_train_preprocessed = np.array([label_map[tuple(label)] for label in y_train], dtype=object)\n",
    "\n",
    "# Map target values to label indices in testing data\n",
    "y_test_preprocessed = np.array([label_map[tuple(label)] for label in y_test], dtype=object)\n",
    "\n",
    "# Convert the preprocessed target data to categorical format\n",
    "y_train_categorical = to_categorical(y_train_preprocessed)\n",
    "y_test_categorical = to_categorical(y_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "58b9c5ce-ab82-464b-b8ee-2dff468434af",
    "_uuid": "6c3dfd65-dcf8-4d5c-ae7d-eb8cea1ef519",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:09.915624Z",
     "iopub.status.busy": "2023-06-23T03:58:09.909121Z",
     "iopub.status.idle": "2023-06-23T03:58:09.928851Z",
     "shell.execute_reply": "2023-06-23T03:58:09.928002Z",
     "shell.execute_reply.started": "2023-06-23T03:58:09.915587Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "032f47fb-d20e-4adc-b7ec-0524af01d995",
    "_uuid": "35ca7b49-2a5c-434a-907f-83d1a3129778",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:09.936044Z",
     "iopub.status.busy": "2023-06-23T03:58:09.933791Z",
     "iopub.status.idle": "2023-06-23T03:58:11.156161Z",
     "shell.execute_reply": "2023-06-23T03:58:11.155209Z",
     "shell.execute_reply.started": "2023-06-23T03:58:09.936012Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_units = 200  # Adjust the number of output units for wideband data\n",
    "model = scale_up_plus_model(input_shape=(None, 1), output_units=output_units, num_stages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "6f89fe4a-32b6-4909-9db8-64f468294c71",
    "_uuid": "dd6defbf-1c05-4c01-bd85-7c67039064f6",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.158184Z",
     "iopub.status.busy": "2023-06-23T03:58:11.157783Z",
     "iopub.status.idle": "2023-06-23T03:58:11.167701Z",
     "shell.execute_reply": "2023-06-23T03:58:11.166741Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.158136Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def Loss_time_total(y_true, y_pred):\n",
    "    weights = [1.0, 2.0, 3.0]  # Example weights for each stage (adjust as needed)\n",
    "    loss = 0.0\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        # Calculate the time loss for each stage\n",
    "        stage_loss = Loss_time(y_true[:, i], y_pred[:, i])  # Slice the tensors to get the appropriate dimensions\n",
    "        weighted_loss = w * stage_loss\n",
    "        loss += weighted_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Loss_frequency_total(y_true, y_pred):\n",
    "    weights = [0.5, 1.0, 1.5]  # Example weights for each stage (adjust as needed)\n",
    "    loss = 0.0\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        # Calculate the frequency loss for each stage\n",
    "        stage_loss = Loss_frequency(y_true[:, i], y_pred[:, i])  # Slice the tensors to get the appropriate dimensions\n",
    "        weighted_loss = w * stage_loss\n",
    "        loss += weighted_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Loss_time(y_true, y_pred):\n",
    "    # Custom time loss calculation\n",
    "    loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Loss_frequency(y_true, y_pred):\n",
    "    # Custom frequency loss calculation\n",
    "    loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b87ba7c8-3c4d-4ffa-b79b-12f927ce7c70",
    "_uuid": "451ef425-c05a-4644-bcbd-10aa0d1c7664",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.171770Z",
     "iopub.status.busy": "2023-06-23T03:58:11.170834Z",
     "iopub.status.idle": "2023-06-23T03:58:11.194689Z",
     "shell.execute_reply": "2023-06-23T03:58:11.193518Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.171733Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=Loss_time, metrics=[Loss_frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "876a7de8-9695-4ea0-9657-bf12616b0bb0",
    "_uuid": "453fe24e-bd8c-426a-ae81-4c26884df6f4",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.197528Z",
     "iopub.status.busy": "2023-06-23T03:58:11.196558Z",
     "iopub.status.idle": "2023-06-23T03:58:11.205190Z",
     "shell.execute_reply": "2023-06-23T03:58:11.204300Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.197496Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up checkpoints to save the best model during training\n",
    "checkpoint = ModelCheckpoint(save_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "8d919322-a9cc-4bc1-92d6-4b2d7b045ec4",
    "_uuid": "a44b58cf-ad3d-43df-a03f-a2faedd2b6ee",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.209099Z",
     "iopub.status.busy": "2023-06-23T03:58:11.206586Z",
     "iopub.status.idle": "2023-06-23T03:58:11.625208Z",
     "shell.execute_reply": "2023-06-23T03:58:11.624238Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.209068Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences in X_train\n",
    "X_train_padded = pad_sequences(X_train, padding='post')\n",
    "X_train_padded = X_train_padded.reshape(X_train_padded.shape[0], X_train_padded.shape[1], 1)\n",
    "\n",
    "# Pad sequences in X_test\n",
    "X_test_padded = pad_sequences(X_test, padding='post')\n",
    "X_test_padded = X_test_padded.reshape(X_test_padded.shape[0], X_test_padded.shape[1], 1)\n",
    "\n",
    "# Convert X_train to a TensorFlow tensor\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_padded, dtype=tf.float32)\n",
    "\n",
    "# Convert y_train_categorical to a TensorFlow tensor\n",
    "y_train_categorical_tensor = tf.convert_to_tensor(y_train_categorical, dtype=tf.float32)\n",
    "\n",
    "# Convert X_test to a TensorFlow tensor\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_padded, dtype=tf.float32)\n",
    "\n",
    "# Convert y_test_categorical to a TensorFlow tensor\n",
    "y_test_categorical_tensor = tf.convert_to_tensor(y_test_categorical, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "694e52c2-0a88-47ab-b136-fa13e4482672",
    "_uuid": "78c4cde7-23e5-4406-8cf0-37d6a493a2df",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.626889Z",
     "iopub.status.busy": "2023-06-23T03:58:11.626501Z",
     "iopub.status.idle": "2023-06-23T03:58:11.634133Z",
     "shell.execute_reply": "2023-06-23T03:58:11.632185Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.626857Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_categorical = to_categorical(y_train_preprocessed, num_classes=output_units)\n",
    "y_test_categorical = to_categorical(y_test_preprocessed, num_classes=output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "06c6a3d2-b257-44ab-bb6f-64d200df74bf",
    "_uuid": "c1b568db-b362-41c2-b0a3-ac9bbc0ed62e",
    "execution": {
     "iopub.execute_input": "2023-06-23T03:58:11.636294Z",
     "iopub.status.busy": "2023-06-23T03:58:11.635623Z",
     "iopub.status.idle": "2023-06-23T03:59:48.601027Z",
     "shell.execute_reply": "2023-06-23T03:59:48.599881Z",
     "shell.execute_reply.started": "2023-06-23T03:58:11.636264Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 1: val_loss improved from inf to 0.01501, saving model to /kaggle/working/\n",
      "171/171 [==============================] - 31s 102ms/step - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 2: val_loss did not improve from 0.01501\n",
      "171/171 [==============================] - 13s 76ms/step - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 3: val_loss did not improve from 0.01501\n",
      "171/171 [==============================] - 13s 75ms/step - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 4: val_loss did not improve from 0.01501\n",
      "171/171 [==============================] - 13s 76ms/step - loss: 0.0150 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0149 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 5: val_loss did not improve from 0.01501\n",
      "171/171 [==============================] - 13s 77ms/step - loss: 0.0149 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0149 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 6: val_loss did not improve from 0.01501\n",
      "171/171 [==============================] - 13s 77ms/step - loss: 0.0149 - conv1d_5_loss: 0.0050 - conv1d_10_loss: 0.0050 - conv1d_15_loss: 0.0050 - conv1d_5_Loss_frequency: 0.0050 - conv1d_10_Loss_frequency: 0.0050 - conv1d_15_Loss_frequency: 0.0050 - val_loss: 0.0150 - val_conv1d_5_loss: 0.0050 - val_conv1d_10_loss: 0.0050 - val_conv1d_15_loss: 0.0050 - val_conv1d_5_Loss_frequency: 0.0050 - val_conv1d_10_Loss_frequency: 0.0050 - val_conv1d_15_Loss_frequency: 0.0050\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "with tf.device(device):\n",
    "    model.fit(X_train_tensor, y_train_categorical, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(X_test_tensor, y_test_categorical), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f511c67d-f7ec-4311-9992-354e104ec1bc",
    "_uuid": "6958c936-5cdd-496f-aa1f-8a93a32fe7bb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
